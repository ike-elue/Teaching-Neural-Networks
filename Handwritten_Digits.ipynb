{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handwritten Digits",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "OAEbxFya1bGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FIRST 3 CELLS IS A COMMON MISTAKE, LAST 3 WORK AND INTRODUCE IDEA OF OVERFITTING DATA, NORMILIZING DATA, INCREASING BRAIN POWER DOESN'T WORK LIKE YOU'D THINK (HIDDEN NEURONS), LEARNING RATE, AND BIAS\n",
        "\n",
        "Helpful Links:\n",
        "- https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/datasets/digits.csv\n",
        "- https://www.kdnuggets.com/2017/08/37-reasons-neural-network-not-working.html/2\n",
        "- https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10\n",
        "- https://stats.stackexchange.com/questions/338255/what-is-effect-of-increasing-number-of-hidden-layers-in-a-feed-forward-nn\n",
        "- https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn\n",
        "- https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc"
      ]
    },
    {
      "metadata": {
        "id": "wYXzSodHGM_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZkSbTkjapBca",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# when it imports, the first row is counted as column data (had to do complicated stuff because of this)\n",
        "dataset = pd.read_csv(\"https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/datasets/digits.csv\")\n",
        "datasetArray = np.array(dataset)\n",
        "xData = datasetArray.T[0:-1]\n",
        "xData = xData.T[0:1]\n",
        "print(xData.shape)\n",
        "yData = datasetArray.T[-1]\n",
        "yData = yData.T[0:xData.shape[0]]\n",
        "print(yData.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "86dGo9uL_rq8",
        "colab_type": "code",
        "outputId": "3cf8626c-14d1-49fb-ab35-1d40ee87c51c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y, hiddenNeurons=25):\n",
        "        self.input = x\n",
        "        np.random.seed(1)\n",
        "        self.weights1 = 2 * np.random.rand(self.input.shape[1],hiddenNeurons) - 1 # 16 x 25\n",
        "        if len(y.shape) == 2:\n",
        "          self.weights2 = 2 * np.random.rand(hiddenNeurons,y.shape[1]) - 1   # 25 x 1\n",
        "        else:\n",
        "          self.weights2 = 2 * np.random.rand(hiddenNeurons, 1) - 1\n",
        "        self.y = y\n",
        "        self.output = np.zeros(self.y.shape)\n",
        "\n",
        "    def activate(self,x,deriv=False):\n",
        "        if(deriv==True):\n",
        "              return x*(1-x)\n",
        "        return 1/(1+np.exp(-x))\n",
        "    \n",
        "    def feedforward(self,x=None):\n",
        "        if(x is None):\n",
        "          self.layer1 = self.activate(np.dot(self.input, self.weights1)) # 10991 x 16 * 16 x 25 = 10991 x 25 (layer 1)\n",
        "        else:\n",
        "          self.layer1 = self.activate(np.dot(x, self.weights1))\n",
        "        self.output = self.activate(np.dot(self.layer1, self.weights2)) # 10991 x 25 * 25 x 1 = 10991 x 1 (output)\n",
        "        \n",
        "    def backprop(self):\n",
        "        errorDerivative = 2*(self.y[:,None] - self.output) * activate(self.output, True)\n",
        "        d_weights2 = np.dot(self.layer1.T, errorDerivative) # 25 x 10991 * 10991 x 1 = 25 x 1 (dweights 2)\n",
        "        deltaHelp = np.dot(errorDerivative, self.weights2.T) * activate(self.layer1, True)\n",
        "        d_weights1 = np.dot(self.input.T,  deltaHelp) # 16 x 10991 * 10991 x 25 = 16 x 25 (dweights 1)\n",
        "        self.weights1 += d_weights1\n",
        "        self.weights2 += d_weights2\n",
        "network = NeuralNetwork(xData,yData)\n",
        "for i in range(6000):\n",
        "    network.feedforward()\n",
        "    if(i % 1000 == 0):\n",
        "      error = np.sum((network.y[:,None] - network.output) ** 2)\n",
        "      print(\"Error: \" + str(error))\n",
        "    network.backprop()\n",
        "toPredict = xData[0]\n",
        "network.feedforward(toPredict)\n",
        "print(str(network.output) + \" when \" + str(toPredict) +  \" is predicted compared to \" + str(yData[0]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 58.93607351341663\n",
            "Error: 49.00009996429573\n",
            "Error: 49.000049990436146\n",
            "Error: 49.000033328917034\n",
            "Error: 49.000024997449714\n",
            "Error: 49.000019998335006\n",
            "[0.99999881] when [ 80 100  18  98  60  66 100  29  42   0   0  23  42  61  56  98] is predicted compared to 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3P04vtQYhDHM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn import preprocessing as pr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULN2LoAshdiw",
        "colab_type": "code",
        "outputId": "ce1ffc22-933b-4b6a-e49e-eea2926cce2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# when it imports, the first row is counted as column data (had to do complicated stuff because of this)\n",
        "dataset = pd.read_csv(\"https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/datasets/digits.csv\")\n",
        "datasetArray = np.array(dataset)\n",
        "xfData = datasetArray.T[0:-1].T\n",
        "amountOfData = 150\n",
        "xData = xfData[0:amountOfData]\n",
        "print(xData.shape)\n",
        "yfData = datasetArray.T[-1]\n",
        "yfData = yfData.T[0:xData.shape[0]]\n",
        "yData = np.zeros((yfData.shape[0], 10))\n",
        "print(yData.shape)\n",
        "for i in range(yfData.shape[0]):\n",
        "  yData[i][yfData[i]] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 16)\n",
            "(150, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PySv-2vNhdrU",
        "colab_type": "code",
        "outputId": "197edb54-47fc-4324-fcfd-4c294b83e8a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y, hiddenNeurons=50, learningRate=1, useBias=False, norm=False):\n",
        "      np.random.seed(1)\n",
        "      # Normalize vs Standardize (I want to use normalization beause implementing standardization is more complicated and I believe this works better)\n",
        "      # Between 0 and 1 instead of around 0 \n",
        "      self.norm = norm\n",
        "      if norm:\n",
        "        self.input = pr.normalize(x)\n",
        "      else:\n",
        "        self.input = x\n",
        "      self.weights1 = 2 * np.random.rand(self.input.shape[1],hiddenNeurons) - 1 # 16 x h\n",
        "      if len(y.shape) == 2:\n",
        "        self.weights2 = 2 * np.random.rand(hiddenNeurons,y.shape[1]) - 1   # h x 8\n",
        "      else:\n",
        "        self.weights2 = 2 * np.random.rand(hiddenNeurons, 1) - 1\n",
        "      self.y = y\n",
        "      self.output = np.zeros(self.y.shape)\n",
        "      self.learningRate = learningRate\n",
        "      self.useBias = useBias\n",
        "      if useBias:\n",
        "        self.bias1 = 2 * np.random.rand(hiddenNeurons) - 1\n",
        "        self.bias2 = 2 * np.random.rand(self.weights2.shape[1]) - 1\n",
        "    \n",
        "    def activate(self,x,deriv=False):\n",
        "      if deriv:\n",
        "        return x*(1-x)\n",
        "      return 1/(1+np.exp(-x))    \n",
        "    \n",
        "    def feedforward(self,x=None,index=0):\n",
        "      # To Hidden\n",
        "      if x is None:\n",
        "        if self.useBias:\n",
        "          temp = np.dot(self.input, self.weights1)\n",
        "          for i in range(temp.shape[0]):\n",
        "            temp[i] += self.bias1\n",
        "          self.layer1 = self.activate(temp)    \n",
        "        else:  \n",
        "          self.layer1 = self.activate(np.dot(self.input, self.weights1)) # amount of data (aod) x 16 * 16 x h = aod x h (layer 1)\n",
        "      else:\n",
        "        if self.norm:\n",
        "          x = pr.normalize(x)\n",
        "        if self.useBias:\n",
        "          self.layer1 = self.activate(np.dot(x[index], self.weights1) + self.bias1)    \n",
        "        else:  \n",
        "          self.layer1 = self.activate(np.dot(x[index], self.weights1))\n",
        "\n",
        "      # To Output\n",
        "      if x is None:\n",
        "        if self.useBias:\n",
        "          temp = np.dot(self.layer1, self.weights2)\n",
        "          for i in range(temp.shape[0]):\n",
        "            temp[i] += self.bias2\n",
        "          self.output = self.activate(temp)    \n",
        "        else:\n",
        "          self.output = self.activate(np.dot(self.layer1, self.weights2)) # aod x h * h x 8 = aod x 8 (output)\n",
        "      else:\n",
        "        if self.useBias:\n",
        "          self.output = self.activate(np.dot(self.layer1, self.weights2) + self.bias2)    \n",
        "        else:\n",
        "          self.output = self.activate(np.dot(self.layer1, self.weights2))\n",
        "        \n",
        "    def backprop(self):\n",
        "      deltaHidden = 2*(self.y - self.output) * activate(self.output, True) # aod x 8\n",
        "      d_weights2 = np.dot(self.layer1.T,deltaHidden) # h x aod * aod x 8 = h x 8 (dweights 2)\n",
        "      deltaInput = np.dot(deltaHidden,self.weights2.T) * activate(self.layer1, True) # aod x h\n",
        "      d_weights1 = np.dot(self.input.T,deltaInput) # 16 x aod * aod x h = 16 x h (dweights 1)\n",
        "      self.weights1 += d_weights1 * self.learningRate\n",
        "      self.weights2 += d_weights2 * self.learningRate\n",
        "      if self.useBias:\n",
        "        d_bias2 = np.sum(deltaHidden,axis = 0,keepdims = True)[0]\n",
        "        d_bias1 = np.sum(deltaInput,axis = 0,keepdims = True)[0]\n",
        "        self.bias1 += d_bias1 * self.learningRate\n",
        "        self.bias2 += d_bias2 * self.learningRate\n",
        "\n",
        "def checkOutput(output):\n",
        "  best = 0\n",
        "  for i in range(len(output)):\n",
        "    if output[best] < output[i]:\n",
        "      best = i\n",
        "  return best\n",
        "network = NeuralNetwork(xData,yData,hiddenNeurons=20,learningRate=.01,useBias=True, norm=True)\n",
        "for i in range(60001):\n",
        "    network.feedforward()\n",
        "    if(i % 10000 == 0):\n",
        "      error = np.sum((network.y - network.output) ** 2)\n",
        "      print(\"Error: \" + str(error))\n",
        "    network.backprop()\n",
        "X = xfData\n",
        "for i in range(0, 10, 2):\n",
        "  network.feedforward(X, i)\n",
        "  newOut = np.zeros((1, 10))\n",
        "  newOut[0][datasetArray.T[-1][i]] = 1\n",
        "  print(str(network.output)) \n",
        "  print(\" aka: \" + str(checkOutput(network.output)) + \" when \" + str(xfData[i]) +  \" is predicted compared to \" + str(newOut[0]) + \" aka: \" + str(datasetArray.T[-1][i]))\n",
        "  print(\"Prediction Correct?: \" + str(checkOutput(network.output) == datasetArray.T[-1][i]))\n",
        "  print(\"----------------------------------------\")\n",
        "\n",
        "counterCorrect = 0\n",
        "counterTotal = X.shape[0]\n",
        "for i in range(amountOfData,counterTotal):\n",
        "  network.feedforward(X, i)\n",
        "  if checkOutput(network.output) == datasetArray.T[-1][i]:\n",
        "    counterCorrect += 1\n",
        "print(\"Correct Percentage: \" + str((counterCorrect/counterTotal) * 100) + \"% for the rest of the \" + str(counterTotal - amountOfData) + \" entries\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 540.930854335504\n",
            "Error: 11.153001419076478\n",
            "Error: 10.377123899726339\n",
            "Error: 10.208766315236959\n",
            "Error: 10.140112558922313\n",
            "Error: 10.103845510602586\n",
            "Error: 10.081747343929894\n",
            "[9.87050857e-06 7.31775367e-09 2.86942996e-12 3.90570013e-10\n",
            " 2.45322296e-12 6.03682542e-05 2.34001584e-04 1.16474893e-02\n",
            " 9.95274391e-01 6.84760223e-11]\n",
            " aka: 8 when [ 80 100  18  98  60  66 100  29  42   0   0  23  42  61  56  98] is predicted compared to [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] aka: 8\n",
            "Prediction Correct?: True\n",
            "----------------------------------------\n",
            "[2.34595474e-04 1.11569037e-03 8.31390361e-11 1.74936898e-04\n",
            " 4.11747433e-05 4.60249644e-12 9.47247973e-05 2.06578110e-10\n",
            " 3.09327088e-08 9.99678682e-01]\n",
            " aka: 9 when [ 95  82  71 100  27  77  77  73 100  80  93  42  56  13   0   0] is predicted compared to [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] aka: 9\n",
            "Prediction Correct?: True\n",
            "----------------------------------------\n",
            "[3.22491047e-11 9.94869839e-01 4.80047768e-08 8.64637846e-09\n",
            " 9.32670110e-06 4.31120324e-07 6.70422586e-04 3.04225003e-06\n",
            " 2.28747229e-03 6.02842056e-09]\n",
            " aka: 1 when [ 70 100 100  97  70  81  45  65  30  49  20  33   0  16   0   0] is predicted compared to [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] aka: 1\n",
            "Prediction Correct?: True\n",
            "----------------------------------------\n",
            "[3.34231544e-06 3.79072578e-02 2.28281537e-03 2.96321622e-05\n",
            " 2.38399742e-04 6.19693990e-06 6.84020993e-10 3.70203411e-08\n",
            " 5.51033920e-09 2.61261704e-05]\n",
            " aka: 1 when [  3  71   0  95  45 100 100  99  79  78  48  53  31  24  54   0] is predicted compared to [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] aka: 7\n",
            "Prediction Correct?: False\n",
            "----------------------------------------\n",
            "[1.04010282e-04 2.76218768e-06 4.67085824e-09 1.62540951e-04\n",
            " 8.16698233e-05 3.90778881e-12 6.34958086e-05 8.75036239e-11\n",
            " 1.22108056e-08 9.97948171e-01]\n",
            " aka: 9 when [ 92  95  30 100  34  68  87  89  84  78 100  35  64   0   0  19] is predicted compared to [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] aka: 9\n",
            "Prediction Correct?: True\n",
            "----------------------------------------\n",
            "Correct Percentage: 85.12419252115367% for the rest of the 10841 entries\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}